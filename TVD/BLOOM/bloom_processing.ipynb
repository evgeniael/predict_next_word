{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/TVD/BLOOM/Generations/processed_bloom.json') as json_file:\n",
    "    combined = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_provo_data(input_data):\n",
    "    \"\"\"A function that takes all .json files we created with info for the Provo Corpus\n",
    "    and merges it into one dictionary\"\"\"\n",
    "    \n",
    "    #We merge all information in one dictionary\n",
    "    # Each data point corresponds to all the information relevant to us for a given context in Provo Corpus\n",
    "    joint_dict = {}\n",
    "    \n",
    "    count = 0\n",
    "    for filename in input_data:\n",
    "        f = open(filename)\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        for text_id in data.keys():\n",
    "            if (int(text_id) > 0) & (int(text_id) <= 55):\n",
    "                for word_num in data[text_id].keys():\n",
    "                    joint_dict[count] = data[text_id][word_num]\n",
    "                    joint_dict[count]['original_positioning'] = {'text_id':text_id, 'word_num':word_num}\n",
    "                \n",
    "                    count = count + 1\n",
    "\n",
    "    return joint_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.getcwd(), '/TVD/GPT2/Generations'))\n",
    "\n",
    "input_data = ['Paragraphs-1-1.json', 'Paragraphs-2-2.json', 'Paragraphs-3-3.json',\n",
    "    'Paragraphs-4-4.json', 'Paragraphs-5-9.json', 'Paragraphs-10-14.json', \n",
    "    'Paragraphs-15-19.json', 'Paragraphs-20-24.json', 'Paragraphs-25-29.json',\n",
    "    'Paragraphs-30-34.json', 'Paragraphs-35-39.json', 'Paragraphs-40-44.json', \n",
    "    'Paragraphs-45-47.json', 'Paragraphs-48-50.json', 'Paragraphs-51-53.json',\n",
    "     'Paragraphs-54-55.json']\n",
    "\n",
    "d = get_provo_data(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = d.copy()\n",
    "provo_key = list(a.keys())\n",
    "provo_positioning = [a[i]['original_positioning'] for i in a.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_samples = []\n",
    "original_corpus_words = []\n",
    "bloom_samples = []\n",
    "\n",
    "for context in combined:\n",
    "    a = d[provo_key[0]].copy()\n",
    "    b = d[provo_key[0]].copy()\n",
    "    provo_key = [i for i, s in enumerate(provo_positioning) if str(context[1]) in str(s)]\n",
    "    human_samp= [[x['pred']]*int(x['count']) for x in d[provo_key[0]]['human']]\n",
    "    human_samp = [item for sublist in human_samp for item in sublist]\n",
    "    original_word = d[provo_key[0]]['original']['pred']\n",
    "    human_samples.append(human_samp)\n",
    "    bloom_samples.append(context[3])\n",
    "    original_corpus_words.append(original_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_oracle_without_replacement_disjoint_groups(words, seed, N = 20):\n",
    "    \"\"\"We create two disjoint subsets of the human distribution by sampling without replacement from\n",
    "    the human distribution (the two disjoing subsets can be comprised by either 10 or 20 samples\"\"\"\n",
    "    #Create a list with all human answers in a flattened out list ['are', 'are', 'they', ..., 'one']\n",
    "    random.seed(seed)\n",
    "\n",
    "    #if the length of the list is odd, we remove one element at random to make the list even,\n",
    "    #since we want the two disjoint subsets to be of equal length\n",
    "    if (len(words) % 2 == 1): \n",
    "        remove_word = random.sample(words, 1)\n",
    "        words.remove(remove_word[0])\n",
    "\n",
    "    #We sample the words that will belong in the first subset and create the second subset by removing\n",
    "    #from the full word list the ones sampled in the first subset\n",
    "    subset1 = random.sample(words, N)\n",
    "    if N == 20:\n",
    "        subset2 = words.copy()\n",
    "        for item in subset1:\n",
    "            subset2.remove(item)\n",
    "    elif N == 10:\n",
    "        subset_left = words.copy()\n",
    "        for item in subset1:\n",
    "            subset_left.remove(item)\n",
    "        subset2 = random.sample(subset_left, N)\n",
    "        \n",
    "    return subset1, subset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator_unbiased(words):\n",
    "    \"\"\"For each data point we compute the estimator where the words belong to the unbiased distribution\"\"\"\n",
    "    #Check for failed to generate full-word samples and remove those\n",
    "    fail = [d for d in words if d == 'Failed to generate word']\n",
    "    if len(fail) > 0:\n",
    "        words= [x for x in words if x != 'Failed to generate word']\n",
    "    \n",
    "    words = [x for x in words if str(x) != 'nan']\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_words = dict(Counter(words))\n",
    "    support = list(dict_words.keys())\n",
    "    counts = list(dict_words.values())\n",
    "    probs = torch.Tensor([x/sum(counts) for x in counts])\n",
    "\n",
    "    return support, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ece_data( assess_words, assess_probs, gold_label_words, gold_label_probs):\n",
    "    \"\"\"Considering the estimator distribution, we obtain the word (and its confidence) with the maximum \n",
    "    probability. For computing accuracy we consider if this word matches the true label (which we consider for \n",
    "    both the cases where they are either the original text word and human majority word\"\"\"\n",
    "    gold_label_probs = gold_label_probs.tolist()\n",
    "    gold_label_majority_word = gold_label_words[gold_label_probs.index(max(gold_label_probs))]\n",
    "    \n",
    "    p_max_word = assess_words[torch.argmax(assess_probs).item()]\n",
    "    human_maj = (torch.max(assess_probs).item(), int(p_max_word == gold_label_majority_word)) \n",
    "\n",
    "    return human_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TVD(probs1, probs2):\n",
    "    tvd = torch.sum(torch.abs(probs1 - probs2))/2\n",
    "    return tvd.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(probs):\n",
    "    #For zero probability values of p in p log p, the contribution to entropy is 0, hence we take only\n",
    "    #non zero p values into account\n",
    "    non_zero_probs = probs[probs > 0]\n",
    "    entropy_probs = - torch.sum(torch.multiply(non_zero_probs, torch.log(non_zero_probs)))\n",
    "\n",
    "    return entropy_probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tvd_per_instance_for_unbiased_est_dist_and_oracle(model_words, model_probs, oracle_words, oracle_probs):\n",
    "        \"\"\"Given the distribution (from the model), we retrieve the human distribution for the same words,\n",
    "        and then compute TVD for the instance level\"\"\"\n",
    "\n",
    "        #We know that the items of the model distribution and the oracle distribution are not currently aligned\n",
    "        #Thus, before computing the TVD between them we first need to align the sample space and probabilities between\n",
    "        #the two distributions\n",
    "        human_probs = []\n",
    "        \n",
    "        list_model_probs = model_probs.tolist()\n",
    "        list_words = model_words.copy()\n",
    "\n",
    "        #For the unbiased distributions, the sampled words may not necessarily include all human words. Hence,\n",
    "        # before creating the human distribution, we add to the model one the ones that are missing with a respective \n",
    "        # probability of zero\n",
    "        list_missing = list(set(oracle_words) - set(list_words)) #set of human words that are not in the model distribution words\n",
    "\n",
    "        for missing_word in list_missing:\n",
    "            list_words.append(missing_word)\n",
    "            list_model_probs.append(0)\n",
    "        \n",
    "        #Similarly to the biased dist., we iterate over all words and the human dist. probabilities are either the retrieved\n",
    "        #probability from the oracle dist. or zero\n",
    "        for word in list_words:\n",
    "            try:\n",
    "                index_word = oracle_words.index(word)\n",
    "                human_probs.append(oracle_probs[index_word].item())\n",
    "            except:\n",
    "                human_probs.append(0)\n",
    "\n",
    "        tvd = compute_TVD(torch.Tensor(human_probs), torch.Tensor(list_model_probs))\n",
    "        return(tvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvd_bloom_human = []\n",
    "tvd_bloom_oracle = []\n",
    "tvd_oracle2_human = []\n",
    "tvd_oracle1_human = []\n",
    "\n",
    "ece_bloom_human_maj = []\n",
    "ece_bloom_corpus_word = []\n",
    "ece_bloom_oracle_maj = {}\n",
    "\n",
    "entropy_bloom = []\n",
    "entropy_human = []\n",
    "entropy_oracle = []\n",
    "\n",
    "for k in range(20):\n",
    "    ece_bloom_oracle_maj[k] = []\n",
    "\n",
    "for i in range(len(bloom_samples) - 1):\n",
    "    human_support, human_probs = get_estimator_unbiased(human_samples[i])\n",
    "    bloom_support, bloom_probs = get_estimator_unbiased(bloom_samples[i])\n",
    "    corpus_support = [original_corpus_words[i]]\n",
    "    corpus_probs = torch.Tensor([1])\n",
    "    oracle_1, oracle_2 = sample_oracle_without_replacement_disjoint_groups(human_samples[i], seed =1)\n",
    "    oracle1_support, oracle1_probs = get_estimator_unbiased(oracle_1)\n",
    "    oracle2_support, oracle2_probs = get_estimator_unbiased(oracle_2)\n",
    "\n",
    "    tvd_bloom_human.append(get_tvd_per_instance_for_unbiased_est_dist_and_oracle(bloom_support, bloom_probs, human_support, human_probs ))\n",
    "    tvd_bloom_oracle.append(get_tvd_per_instance_for_unbiased_est_dist_and_oracle(bloom_support, bloom_probs, oracle1_support, oracle1_probs ))\n",
    "    tvd_oracle2_human.append(get_tvd_per_instance_for_unbiased_est_dist_and_oracle(oracle2_support, oracle2_probs, human_support, human_probs ))\n",
    "    tvd_oracle1_human.append(get_tvd_per_instance_for_unbiased_est_dist_and_oracle(oracle1_support, oracle1_probs, human_support, human_probs ))\n",
    "\n",
    "    entropy_bloom.append(compute_entropy(bloom_probs))\n",
    "    entropy_human.append(compute_entropy(human_probs))\n",
    "    entropy_oracle.append(compute_entropy(oracle1_probs))\n",
    "\n",
    "    ece_bloom_human_maj.append(get_ece_data( bloom_support, bloom_probs, human_support, human_probs))\n",
    "    ece_bloom_corpus_word.append(get_ece_data( bloom_support, bloom_probs, corpus_support, corpus_probs))\n",
    "    \n",
    "    for k in range(20):\n",
    "        oracle_1, oracle_2 = sample_oracle_without_replacement_disjoint_groups(human_samples[i], k)\n",
    "        oracle_support, oracle_probs = get_estimator_unbiased(oracle_1)\n",
    "        ece_bloom_oracle_maj[k].append(get_ece_data( bloom_support, bloom_probs, oracle_support, oracle_probs))\n",
    "\n",
    "    #tvd_human_gpt2 = get_tvd_per_instance_for_unbiased_est_dist_and_oracle(gpt2_support, gpt2_probs, human_support, human_probs )\n",
    "\n",
    "    # tvd_values_bloom.append(tvd_human_bloom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6124237598920929"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expected TVD between Humans and Bloom distributions\n",
    "np.mean(tvd_bloom_human) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloom_results = {}\n",
    "\n",
    "bloom_results['tvd_bloom_human'] = tvd_bloom_human\n",
    "bloom_results['tvd_oracle1_human'] = tvd_oracle1_human\n",
    "bloom_results['tvd_oracle2_human'] = tvd_oracle2_human\n",
    "bloom_results['tvd_bloom_oracle'] = tvd_bloom_oracle\n",
    "\n",
    "bloom_results['entropy_bloom'] = entropy_bloom\n",
    "bloom_results['entropy_human'] = entropy_human\n",
    "bloom_results['entropy_oracle'] = entropy_oracle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(bloom_results)\n",
    "\n",
    "with open('TVD_Bloom.json', 'w') as outfile:\n",
    "    outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ECE(conf_acc, ece_bins = 10):\n",
    "    \"\"\"Function that given a list of tuples including the confidence of each prediction and if it matches the\n",
    "    true label computes the ECE. To do that, we split the confidence space (0,1) in bins, separate predictions\n",
    "    according to the bins, calculate the average confidence per bin and the accuracy per bin and take their weighted\n",
    "    average.\"\"\"\n",
    "    bins = ece_bins\n",
    "    conf_acc_array = np.array(conf_acc)\n",
    "    N = conf_acc_array.shape[0]\n",
    "        \n",
    "    sum_bin = 0\n",
    "    for i in np.arange(0, 1, 1/bins):\n",
    "        #getting all points which belong to the relevant bin - given their \n",
    "        bin_contents = conf_acc_array[np.where((conf_acc_array[:,0] >= i) & (conf_acc_array[:,0] < (i + 1/bins)))]\n",
    "        n_bin = bin_contents[:,0].shape[0]\n",
    "        if n_bin > 0: #if the bin is non empty\n",
    "            avg_conf = np.sum(bin_contents[:,0]) / n_bin\n",
    "            acc = np.sum(bin_contents[:,1]) / n_bin\n",
    "            sum_bin = sum_bin + abs(avg_conf - acc) * n_bin / N\n",
    "        \n",
    "    ece_val = sum_bin\n",
    "    return(ece_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08917578235714069"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get ECE for BLOOM using the human majority\n",
    "calculate_ECE(ece_bloom_human_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07100397307754278"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get ECE for BLOOM using the corpus word\n",
    "calculate_ECE(ece_bloom_corpus_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get average ECE and respective standard deviation for BLOOM using the oracle majority accross different runs\n",
    "ece_subsamples = []\n",
    "for i in range(20):\n",
    "    ece_subsamples.append(calculate_ECE(ece_bloom_oracle_maj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007561531481564479"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(ece_subsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07497322483236035"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ece_subsamples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e179d8c04b1f29b078a38886792aa2cef325207e3fb3fd25318ecaf57ba8e2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
